{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Layer Data Processing\n",
    "This notebook creates gold Delta tables from silver data using configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "import os\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"should_test\", \"false\", [\"true\", \"false\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sql_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of SQL files in silver config folder\n",
    "silver_path = \"../configs/silver\"\n",
    "sql_files = [f for f in os.listdir(silver_path) if f.endswith('.sql')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single SQL file and create Delta table\n",
    "def process_sql_file(sql_file):\n",
    "    try:\n",
    "        # Get the table name from file name (remove .sql extension)\n",
    "        table_name = sql_file.replace('.sql', '')\n",
    "        \n",
    "        # Read SQL content\n",
    "        sql_content = read_sql_file(os.path.join(silver_path, sql_file))\n",
    "        \n",
    "        # Create temporary view with unique name to avoid conflicts\n",
    "        temp_view_name = f\"temp_view_{table_name}\"\n",
    "        spark.sql(sql_content).createOrReplaceTempView(temp_view_name)\n",
    "        \n",
    "        # Create Delta table\n",
    "        spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS practice_sandbox.ma_sandbox.silver_{table_name}\n",
    "        USING DELTA\n",
    "        AS\n",
    "        SELECT * FROM {temp_view_name}\n",
    "        \"\"\")\n",
    "        \n",
    "        # Clean up temporary view\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS {temp_view_name}\")\n",
    "        \n",
    "        return f\"Successfully created Delta table: practice_sandbox.ma_sandbox.silver_{table_name}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error processing {sql_file}: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Process SQL files in parallel using ThreadPoolExecutor\n",
    "max_workers = min(len(sql_files), 4)  # Limit max workers to avoid overwhelming the system\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    future_to_sql = {executor.submit(process_sql_file, sql_file): sql_file for sql_file in sql_files}\n",
    "    \n",
    "    for future in as_completed(future_to_sql):\n",
    "        sql_file = future_to_sql[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            print(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {sql_file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all created tables\n",
    "tables = spark.sql(\"SHOW TABLES IN practice_sandbox.ma_sandbox\").filter(\"tableName LIKE 'silver_%'\")\n",
    "display(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_not_null_ids(table_name):\n",
    "    \"\"\"Test that ID columns don't contain nulls\"\"\"\n",
    "    # Get all ID columns (assuming they end with '_id' or are named 'id')\n",
    "    columns_df = spark.sql(f\"DESCRIBE {table_name}\")\n",
    "    id_columns = [row.col_name for row in columns_df.collect() \n",
    "                 if row.col_name.lower().endswith('_id') \n",
    "                 or row.col_name.lower() == 'id']\n",
    "    \n",
    "    results = []\n",
    "    for col in id_columns:\n",
    "        null_count = spark.sql(f\"SELECT COUNT(*) as null_count FROM {table_name} WHERE {col} IS NULL\").collect()[0].null_count\n",
    "        results.append({\n",
    "            'table': table_name,\n",
    "            'column': col,\n",
    "            'test': 'not_null',\n",
    "            'passed': null_count == 0,\n",
    "            'failed_records': null_count\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def test_unique_ids(table_name):\n",
    "    \"\"\"Test that ID columns contain unique values\"\"\"\n",
    "    columns_df = spark.sql(f\"DESCRIBE {table_name}\")\n",
    "    id_columns = [row.col_name for row in columns_df.collect() \n",
    "                 if row.col_name.lower().endswith('_id') \n",
    "                 or row.col_name.lower() == 'id']\n",
    "    \n",
    "    results = []\n",
    "    for col in id_columns:\n",
    "        duplicate_count = spark.sql(f\"\"\"\n",
    "            SELECT COUNT(*) as dup_count \n",
    "            FROM (\n",
    "                SELECT {col}\n",
    "                FROM {table_name}\n",
    "                WHERE {col} IS NOT NULL\n",
    "                GROUP BY {col}\n",
    "                HAVING COUNT(*) > 1\n",
    "            )\n",
    "        \"\"\").collect()[0].dup_count\n",
    "        \n",
    "        results.append({\n",
    "            'table': table_name,\n",
    "            'column': col,\n",
    "            'test': 'unique',\n",
    "            'passed': duplicate_count == 0,\n",
    "            'failed_records': duplicate_count\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_test:\n",
    "    print(\"Running data quality tests...\")\n",
    "    all_test_results = []\n",
    "\n",
    "    tables_list = spark.sql(\"SHOW TABLES IN practice_sandbox.ma_sandbox\").filter(\"tableName LIKE 'silver_%'\").collect()\n",
    "    for table in tables_list:\n",
    "        table_name = f\"practice_sandbox.ma_sandbox.{table.tableName}\"\n",
    "        \n",
    "        # Run not null tests\n",
    "        null_results = test_not_null_ids(table_name)\n",
    "        all_test_results.extend(null_results)\n",
    "        \n",
    "        # Run unique tests\n",
    "        unique_results = test_unique_ids(table_name)\n",
    "        all_test_results.extend(unique_results)\n",
    "\n",
    "    # Create DataFrame with test results\n",
    "    test_results_df = spark.createDataFrame(all_test_results)\n",
    "    display(test_results_df)\n",
    "\n",
    "    # Check if any tests failed\n",
    "    failed_tests = test_results_df.filter(\"passed = false\").count()\n",
    "    if failed_tests > 0:\n",
    "        print(f\"\\n⚠️ {failed_tests} tests failed! Check the results above for details.\")\n",
    "    else:\n",
    "        print(\"\\n✅ All tests passed!\")\n",
    "else:\n",
    "    print(\"Skipping tests as should_test is set to false\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
