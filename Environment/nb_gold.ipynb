{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d344d72a-beda-45da-bcc5-00553d66c8cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Gold Layer Data Processing\n",
    "This notebook creates gold Delta tables from silver data using configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4d8f1e7-b2ea-4aa3-9aa7-54114add09e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "import os\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba82dc21-8aa7-4b83-a5e6-8c4de4c987be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"should_test\", \"false\", [\"true\", \"false\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7fc487a-a26c-4b66-b5e4-e7edf4c68d03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "should_test = dbutils.widgets.get(\"should_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb163871-8d13-42a9-b95f-e723fc45474b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_sql_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4112da7-080c-41f6-b621-989a7e3bcd0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get list of SQL files in gold config folder\n",
    "gold_path = \"../configs/gold\"\n",
    "sql_files = [f for f in os.listdir(gold_path) if f.endswith('.sql')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7724218-3609-4ca9-bc6f-37f89dcfee9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to process a single SQL file and create Delta table\n",
    "def process_sql_file(sql_file):\n",
    "    try:\n",
    "        # Get the table name from file name (remove .sql extension)\n",
    "        table_name = sql_file.replace('.sql', '')\n",
    "        \n",
    "        # Read SQL content\n",
    "        sql_content = read_sql_file(os.path.join(gold_path, sql_file))\n",
    "        \n",
    "        # Create temporary view with unique name to avoid conflicts\n",
    "        temp_view_name = f\"temp_view_{table_name}\"\n",
    "        spark.sql(sql_content).createOrReplaceTempView(temp_view_name)\n",
    "        \n",
    "        # Create Delta table\n",
    "        spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS practice_sandbox.ma_sandbox.gold_{table_name}\n",
    "        USING DELTA\n",
    "        AS\n",
    "        SELECT * FROM {temp_view_name}\n",
    "        \"\"\")\n",
    "        \n",
    "        # Clean up temporary view\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS {temp_view_name}\")\n",
    "        \n",
    "        return f\"Successfully created Delta table: practice_sandbox.ma_sandbox.gold_{table_name}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error processing {sql_file}: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b6e5876-fefb-49fc-ab78-4d0014800bb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Process SQL files in parallel using ThreadPoolExecutor\n",
    "max_workers = min(len(sql_files), 4)  # Limit max workers to avoid overwhelming the system\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    future_to_sql = {executor.submit(process_sql_file, sql_file): sql_file for sql_file in sql_files}\n",
    "    \n",
    "    for future in as_completed(future_to_sql):\n",
    "        sql_file = future_to_sql[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            print(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {sql_file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "932fd53d-6010-40c7-bb74-713a8cd3c730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List all created tables\n",
    "tables = spark.sql(\"SHOW TABLES IN practice_sandbox.ma_sandbox\").filter(\"tableName LIKE 'gold_%'\")\n",
    "display(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d390684-1ab0-4353-9346-40c957335a10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_not_null_ids(table_name):\n",
    "    \"\"\"Test that ID columns don't contain nulls\"\"\"\n",
    "    # Get all ID columns (assuming they end with '_id' or are named 'id')\n",
    "    columns_df = spark.sql(f\"DESCRIBE {table_name}\")\n",
    "    id_columns = [row.col_name for row in columns_df.collect() \n",
    "                 if row.col_name.lower().endswith('_id') \n",
    "                 or row.col_name.lower() == 'id']\n",
    "    \n",
    "    results = []\n",
    "    for col in id_columns:\n",
    "        null_count = spark.sql(f\"SELECT COUNT(*) as null_count FROM {table_name} WHERE {col} IS NULL\").collect()[0].null_count\n",
    "        results.append({\n",
    "            'table': table_name,\n",
    "            'column': col,\n",
    "            'test': 'not_null',\n",
    "            'passed': null_count == 0,\n",
    "            'failed_records': null_count\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def test_unique_ids(table_name):\n",
    "    \"\"\"Test that ID columns contain unique values\"\"\"\n",
    "    columns_df = spark.sql(f\"DESCRIBE {table_name}\")\n",
    "    id_columns = [row.col_name for row in columns_df.collect() \n",
    "                 if row.col_name.lower().endswith('_id') \n",
    "                 or row.col_name.lower() == 'id']\n",
    "    \n",
    "    results = []\n",
    "    for col in id_columns:\n",
    "        duplicate_count = spark.sql(f\"\"\"\n",
    "            SELECT COUNT(*) as dup_count \n",
    "            FROM (\n",
    "                SELECT {col}\n",
    "                FROM {table_name}\n",
    "                WHERE {col} IS NOT NULL\n",
    "                GROUP BY {col}\n",
    "                HAVING COUNT(*) > 1\n",
    "            )\n",
    "        \"\"\").collect()[0].dup_count\n",
    "        \n",
    "        results.append({\n",
    "            'table': table_name,\n",
    "            'column': col,\n",
    "            'test': 'unique',\n",
    "            'passed': duplicate_count == 0,\n",
    "            'failed_records': duplicate_count\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46ea5508-9135-480f-b1af-ace75b6fc5e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if should_test:\n",
    "    print(\"Running data quality tests...\")\n",
    "    all_test_results = []\n",
    "\n",
    "    tables_list = spark.sql(\"SHOW TABLES IN practice_sandbox.ma_sandbox\").filter(\"tableName LIKE 'gold_%'\").collect()\n",
    "    for table in tables_list:\n",
    "        table_name = f\"practice_sandbox.ma_sandbox.{table.tableName}\"\n",
    "        \n",
    "        # Run not null tests\n",
    "        null_results = test_not_null_ids(table_name)\n",
    "        all_test_results.extend(null_results)\n",
    "        \n",
    "        # Run unique tests\n",
    "        unique_results = test_unique_ids(table_name)\n",
    "        all_test_results.extend(unique_results)\n",
    "\n",
    "    # Create DataFrame with test results\n",
    "    test_results_df = spark.createDataFrame(all_test_results)\n",
    "    display(test_results_df)\n",
    "\n",
    "    # Check if any tests failed\n",
    "    failed_tests = test_results_df.filter(\"passed = false\").count()\n",
    "    if failed_tests > 0:\n",
    "        print(f\"\\n⚠️ {failed_tests} tests failed! Check the results above for details.\")\n",
    "    else:\n",
    "        print(\"\\n✅ All tests passed!\")\n",
    "else:\n",
    "    print(\"Skipping tests as should_test is set to false\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_gold",
   "widgets": {
    "should_test": {
     "currentValue": "false",
     "nuid": "ed2aac83-13fa-4aa7-a34b-d0535c4c8ea1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": null,
      "name": "should_test",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "true",
        "false"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "false",
      "label": null,
      "name": "should_test",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "true",
        "false"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
