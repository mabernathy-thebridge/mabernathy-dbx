{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d344d72a-beda-45da-bcc5-00553d66c8cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Gold Layer Data Processing\n",
    "This notebook creates gold Delta tables from silver data using configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4d8f1e7-b2ea-4aa3-9aa7-54114add09e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "import os\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba82dc21-8aa7-4b83-a5e6-8c4de4c987be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"should_test\", \"false\", [\"true\", \"false\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7fc487a-a26c-4b66-b5e4-e7edf4c68d03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "should_test = dbutils.widgets.get(\"should_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb163871-8d13-42a9-b95f-e723fc45474b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_sql_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4112da7-080c-41f6-b621-989a7e3bcd0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get list of SQL files in gold config folder\n",
    "gold_path = \"./configs/gold\"\n",
    "sql_files = [f for f in os.listdir(gold_path) if f.endswith('.sql')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7724218-3609-4ca9-bc6f-37f89dcfee9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read and execute each view definition from configs/gold\n",
    "gold_path = Path('configs/gold')\n",
    "for sql_file in gold_path.glob('*.sql'):\n",
    "    with open(sql_file, 'r') as f:\n",
    "        view_definition = f.read()\n",
    "        spark.sql(view_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "932fd53d-6010-40c7-bb74-713a8cd3c730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List all created views\n",
    "views = spark.sql(\"SHOW VIEWS IN practice_sandbox.ma_sandbox\").filter(\"viewName LIKE 'gold_%'\")\n",
    "display(views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d390684-1ab0-4353-9346-40c957335a10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_not_null_ids(view_name):\n",
    "    \"\"\"Test that ID columns don't contain nulls\"\"\"\n",
    "    # Get all ID columns (assuming they end with '_id' or are named 'id')\n",
    "    columns_df = spark.sql(f\"DESCRIBE {view_name}\")\n",
    "    id_columns = [row.col_name for row in columns_df.collect() \n",
    "                 if row.col_name.lower().endswith('_id') \n",
    "                 or row.col_name.lower() == 'id']\n",
    "    \n",
    "    results = []\n",
    "    for col in id_columns:\n",
    "        null_count = spark.sql(f\"SELECT COUNT(*) as null_count FROM {view_name} WHERE {col} IS NULL\").collect()[0].null_count\n",
    "        results.append({\n",
    "            'view': view_name,\n",
    "            'column': col,\n",
    "            'test': 'not_null',\n",
    "            'passed': null_count == 0,\n",
    "            'failed_records': null_count\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def test_unique_ids(view_name):\n",
    "    \"\"\"Test that ID columns contain unique values\"\"\"\n",
    "    columns_df = spark.sql(f\"DESCRIBE {view_name}\")\n",
    "    id_columns = [row.col_name for row in columns_df.collect() \n",
    "                 if row.col_name.lower().endswith('_id') \n",
    "                 or row.col_name.lower() == 'id']\n",
    "    \n",
    "    results = []\n",
    "    for col in id_columns:\n",
    "        duplicate_count = spark.sql(f\"\"\"\n",
    "            SELECT COUNT(*) as dup_count \n",
    "            FROM (\n",
    "                SELECT {col}\n",
    "                FROM {view_name}\n",
    "                WHERE {col} IS NOT NULL\n",
    "                GROUP BY {col}\n",
    "                HAVING COUNT(*) > 1\n",
    "            )\n",
    "        \"\"\").collect()[0].dup_count\n",
    "        \n",
    "        results.append({\n",
    "            'view': view_name,\n",
    "            'column': col,\n",
    "            'test': 'unique',\n",
    "            'passed': duplicate_count == 0,\n",
    "            'failed_records': duplicate_count\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46ea5508-9135-480f-b1af-ace75b6fc5e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if should_test:\n",
    "    print(\"Running data quality tests...\")\n",
    "    all_test_results = []\n",
    "\n",
    "    views_list = spark.sql(\"SHOW VIEWS IN practice_sandbox.ma_sandbox\").filter(\"viewName LIKE 'gold_%'\").collect()\n",
    "    for view in views_list:\n",
    "        view_name = f\"practice_sandbox.ma_sandbox.{view.viewName}\"\n",
    "        \n",
    "        # Run not null tests\n",
    "        null_results = test_not_null_ids(view_name)\n",
    "        all_test_results.extend(null_results)\n",
    "        \n",
    "        # Run unique tests\n",
    "        unique_results = test_unique_ids(view_name)\n",
    "        all_test_results.extend(unique_results)\n",
    "\n",
    "    # Create DataFrame with test results\n",
    "    test_results_df = spark.createDataFrame(all_test_results)\n",
    "    display(test_results_df)\n",
    "\n",
    "    # Check if any tests failed\n",
    "    failed_tests = test_results_df.filter(\"passed = false\").count()\n",
    "    if failed_tests > 0:\n",
    "        print(f\"\\n⚠️ {failed_tests} tests failed! Check the results above for details.\")\n",
    "    else:\n",
    "        print(\"\\n✅ All tests passed!\")\n",
    "else:\n",
    "    print(\"Skipping tests as should_test is set to false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99c21a05-0508-45c4-a4a8-f00147a5a383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_gold",
   "widgets": {
    "should_test": {
     "currentValue": "false",
     "nuid": "ed2aac83-13fa-4aa7-a34b-d0535c4c8ea1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": null,
      "name": "should_test",
      "options": {
       "choices": [
        "true",
        "false"
       ],
       "fixedDomain": true,
       "multiselect": false,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "false",
      "label": null,
      "name": "should_test",
      "options": {
       "autoCreated": null,
       "choices": [
        "true",
        "false"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
